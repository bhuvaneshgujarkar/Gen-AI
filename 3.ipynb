{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04d38cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d5c5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ba02578d0142b69dd0f6ea1fadbfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhuva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bhuva\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ebf592ea224085acd267003dd964a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f215a81c90f4ef5b2f8e7dec97ed214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24a832a427c4d08bc828261cdd6c80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ff775319ad41c8b768c312e59ea7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Exploring Tokenization:\n",
      "==================================================\n",
      "\n",
      "Original: Hello world!\n",
      "Tokens: ['Hello', 'Ä world', '!']\n",
      "Token IDs: [15496, 995, 0]\n",
      "Number of tokens: 3\n",
      "\n",
      "Original: The quick brown fox jumps over the lazy dog.\n",
      "Tokens: ['The', 'Ä quick', 'Ä brown', 'Ä fox', 'Ä jumps', 'Ä over', 'Ä the', 'Ä lazy', 'Ä dog', '.']\n",
      "Token IDs: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
      "Number of tokens: 10\n",
      "\n",
      "Original: Artificial intelligence is revolutionizing technology.\n",
      "Tokens: ['Art', 'ificial', 'Ä intelligence', 'Ä is', 'Ä revolution', 'izing', 'Ä technology', '.']\n",
      "Token IDs: [8001, 9542, 4430, 318, 5854, 2890, 3037, 13]\n",
      "Number of tokens: 8\n",
      "\n",
      "Original: GPT-2 uses transformer architecture.\n",
      "Tokens: ['G', 'PT', '-', '2', 'Ä uses', 'Ä transformer', 'Ä architecture', '.']\n",
      "Token IDs: [38, 11571, 12, 17, 3544, 47385, 10959, 13]\n",
      "Number of tokens: 8\n",
      "\n",
      "Original: Supercalifragilisticexpialidocious\n",
      "Tokens: ['Super', 'cal', 'if', 'rag', 'il', 'ist', 'ice', 'xp', 'ial', 'id', 'ocious']\n",
      "Token IDs: [12442, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\n",
      "Number of tokens: 11\n",
      "\n",
      "ðŸ“Š Tokenizer vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Test sentences to explore tokenization\n",
    "test_sentences = [\n",
    "    \"Hello world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is revolutionizing technology.\",\n",
    "    \"GPT-2 uses transformer architecture.\",\n",
    "    \"Supercalifragilisticexpialidocious\"  # Long word to see sub-word tokenization\n",
    "]\n",
    "\n",
    "print(\"ðŸ” Exploring Tokenization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    # TODO: Tokenize the sentence\n",
    "    # Hint: Use tokenizer.encode() to get token IDs\n",
    "    # Use tokenizer.tokenize() to see the actual tokens\n",
    "    tokens = tokenizer.tokenize(sentence)  # Get the actual token strings\n",
    "    token_ids = tokenizer.encode(sentence)  # Get the numerical IDs\n",
    "    \n",
    "    print(f\"\\nOriginal: {sentence}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# TODO: Print tokenizer vocabulary size\n",
    "print(f\"\\nðŸ“Š Tokenizer vocabulary size: {tokenizer.vocab_size}\")  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cfc5d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running                        â†’ ['running'] (1 tokens)\n",
      "runner                         â†’ ['runner'] (1 tokens)\n",
      "run                            â†’ ['run'] (1 tokens)\n",
      "unhappiness                    â†’ ['un', 'h', 'appiness'] (3 tokens)\n",
      "ChatGPT                        â†’ ['Chat', 'G', 'PT'] (3 tokens)\n",
      "COVID-19                       â†’ ['CO', 'VID', '-', '19'] (4 tokens)\n",
      "2023                           â†’ ['20', '23'] (2 tokens)\n",
      "programming                    â†’ ['program', 'ming'] (2 tokens)\n",
      "antidisestablishmentarianism   â†’ ['ant', 'idis', 'establishment', 'arian', 'ism'] (5 tokens)\n",
      "\n",
      "ðŸ“Š Average characters per token: 4.12\n",
      "ðŸ“Š Longest word in tokens: antidisestablishmentarianism\n"
     ]
    }
   ],
   "source": [
    "analysis_texts = [\n",
    "    \"running\",\n",
    "    \"runner\", \n",
    "    \"run\",\n",
    "    \"unhappiness\",\n",
    "    \"ChatGPT\",\n",
    "    \"COVID-19\",\n",
    "    \"2023\",\n",
    "    \"programming\",\n",
    "    \"antidisestablishmentarianism\"\n",
    "]\n",
    "token_analysis = []\n",
    "for text in analysis_texts:\n",
    "    tokens = tokenizer.tokenize(text)  # Tokenize the text\n",
    "    token_count = len(tokens)  # Count the tokens\n",
    "    token_analysis.append({\n",
    "        'text': text,\n",
    "        'tokens': tokens,\n",
    "        'token_count': token_count,\n",
    "        'chars_per_token': len(text) / token_count\n",
    "    })\n",
    "    print(f\"{text:30} â†’ {tokens} ({token_count} tokens)\")\n",
    "df = pd.DataFrame(token_analysis)\n",
    "print(f\"\\nðŸ“Š Average characters per token: {df['chars_per_token'].mean():.2f}\")  # Calculate mean\n",
    "print(f\"ðŸ“Š Longest word in tokens: {df.loc[df['token_count'].idxmax(), 'text']}\")  # Find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "182bed0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading GPT-2 model (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcfa18947154bf29857d017b50cd78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997b3677390646b28d78c7df8cfc2127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPT-2 model loaded successfully!\n",
      "\n",
      "ðŸ—ï¸ Model Architecture:\n",
      "Model type: GPT2LMHeadModel\n",
      "Total parameters: 124,439,808\n",
      "Model size: ~124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load GPT-2 model\n",
    "# Hint: Use GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "print(\"ðŸ”„ Loading GPT-2 model (this may take a moment)...\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# TODO: Set model to evaluation mode\n",
    "# Hint: Use model.eval()\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… GPT-2 model loaded successfully!\")\n",
    "\n",
    "# Explore model architecture\n",
    "print(\"\\nðŸ—ï¸ Model Architecture:\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "\n",
    "# TODO: Count model parameters\n",
    "# Hint: sum(p.numel() for p in model.parameters())\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Model size: ~{total_params / 1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0eddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Model Structure Analysis:\n",
      "==================================================\n",
      "Vocabulary size: 50257\n",
      "Maximum sequence length: 1024\n",
      "Number of transformer layers: 12\n",
      "Number of attention heads: 12\n",
      "Hidden size: 768\n",
      "\n",
      "ðŸ¤” Comparison to Your Neural Network:\n",
      "Your network had: 2 inputs â†’ 4 hidden â†’ 1 output\n",
      "GPT-2 has: 50257 inputs â†’ 768 hidden â†’ 50257 outputs\n",
      "Your network: ~50 parameters\n",
      "GPT-2: 124,439,808 parameters\n",
      "GPT-2 is ~2,488,796x larger!\n"
     ]
    }
   ],
   "source": [
    "# Explore model structure\n",
    "print(\"ðŸ” Model Structure Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Print model configuration\n",
    "# Hint: Use model.config\n",
    "config = model.config\n",
    "\n",
    "print(f\"Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"Maximum sequence length: {config.n_positions}\")\n",
    "print(f\"Number of transformer layers: {config.n_layer}\")\n",
    "print(f\"Number of attention heads: {config.n_head}\")\n",
    "print(f\"Hidden size: {config.n_embd}\")\n",
    "\n",
    "# Compare to your simple network\n",
    "print(\"\\nðŸ¤” Comparison to Your Neural Network:\")\n",
    "print(f\"Your network had: 2 inputs â†’ 4 hidden â†’ 1 output\")\n",
    "print(f\"GPT-2 has: {config.vocab_size} inputs â†’ {config.n_embd} hidden â†’ {config.vocab_size} outputs\")\n",
    "print(f\"Your network: ~50 parameters\")\n",
    "print(f\"GPT-2: {total_params:,} parameters\")\n",
    "print(f\"GPT-2 is ~{total_params/50:,.0f}x larger!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "555143e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Base prompt: 'In the future, artificial intelligence will'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a text generation pipeline\n",
    "# Hint: Use pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Base prompt for experiments\n",
    "base_prompt = \"In the future, artificial intelligence will\"\n",
    "\n",
    "print(f\"ðŸ¤– Base prompt: '{base_prompt}'\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e470e3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ¡ï¸ Temperature Experiments:\n",
      "==================================================\n",
      "\n",
      "ðŸ”¥ Temperature: 0.1\n",
      "------------------------------\n",
      "In the future, artificial intelligence will be able to do things like search for information about people, and to do things like search for information about people.\n",
      "\n",
      "The future of AI is going to be a lot more complex than we've ever imagined.\n",
      "\n",
      "The future of AI is going to be\n",
      "\n",
      "ðŸ”¥ Temperature: 0.7\n",
      "------------------------------\n",
      "In the future, artificial intelligence will be able to be used in situations where it is necessary to use complex systems in order to solve problems such as cancer, terrorism, etc. These are not always practical issues for AI, and we need to consider how to use them. We will begin by thinking about\n",
      "\n",
      "ðŸ”¥ Temperature: 1.0\n",
      "------------------------------\n",
      "In the future, artificial intelligence will be able to replace traditional social networks by making it faster and smarter -- in the same way that Google and AT&T can help you to search a supermarket.\n",
      "\n",
      "In fact, by 2030, the government plan calls for artificial intelligence in all areas of society to\n",
      "\n",
      "ðŸ”¥ Temperature: 1.5\n",
      "------------------------------\n",
      "In the future, artificial intelligence will also improve efficiency if computers can efficiently work faster.\"\n",
      "\n",
      "There is, indeed, evidence that neural connections are an important resource even when done slowly and not fully integrated. A team led by Martin and Ostrovsky in the early work described how they've taken\n",
      "\n",
      "ðŸ¤” Discussion Questions:\n",
      "â€¢ Which temperature produced the most coherent text?\n",
      "â€¢ Which was most creative/surprising?\n",
      "â€¢ When might you use each temperature setting?\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different temperatures\n",
    "temperatures = [0.1, 0.7, 1.0, 1.5]\n",
    "\n",
    "print(\"ðŸŒ¡ï¸ Temperature Experiments:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nðŸ”¥ Temperature: {temp}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # TODO: Generate text with different temperatures\n",
    "    # Hint: Use generator() with temperature parameter\n",
    "    result = generator(\n",
    "        base_prompt,  # prompt\n",
    "        max_length=60,  # try 60\n",
    "        temperature=temp,  # use the temp variable\n",
    "        do_sample=True,  # should be True for sampling\n",
    "        pad_token_id=tokenizer.eos_token_id  # use tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # TODO: Print the generated text\n",
    "    generated_text = result[0]['generated_text']  # Extract from result\n",
    "    print(generated_text)\n",
    "    \n",
    "print(\"\\nðŸ¤” Discussion Questions:\")\n",
    "print(\"â€¢ Which temperature produced the most coherent text?\")\n",
    "print(\"â€¢ Which was most creative/surprising?\")\n",
    "print(\"â€¢ When might you use each temperature setting?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c37c552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Top-p Sampling Experiments:\n",
      "==================================================\n",
      "\n",
      "ðŸŽ² Top-p: 0.3\n",
      "------------------------------\n",
      "In the future, artificial intelligence will be able to learn from our past and learn from our future.\n",
      "\n",
      "In the future, artificial intelligence will be able to learn from our past and learn from our future.\n",
      "\n",
      "In the future, artificial intelligence will be able to learn from our past and learn\n",
      "\n",
      "ðŸŽ² Top-p: 0.7\n",
      "------------------------------\n",
      "In the future, artificial intelligence will be able to build machines that are as accurate as human eyes. This is a big step forward for AI, but it's not enough. We need to get smarter.\n",
      "\n",
      "The world's next generation of AI is going to be much more complex than we've\n",
      "\n",
      "ðŸŽ² Top-p: 0.9\n",
      "------------------------------\n",
      "In the future, artificial intelligence will be able to understand the natural world better than humans can.\n",
      "\n",
      "\"It's an exciting day for AI because we've discovered that the human brain is able to understand the world better than anything else,\" said Dr. C.K. Johnson, an assistant professor\n",
      "\n",
      "ðŸŽ² Top-p: 1.0\n",
      "------------------------------\n",
      "In the future, artificial intelligence will be able to help us better understand the human brain.\n",
      "\n",
      "The challenge is that it's impossible to solve without human help.\n",
      "\n",
      "\"The problem is that humans don't even know how things work and we don't know if they could do it, so\n",
      "\n",
      "ðŸ¤” Discussion Questions:\n",
      "â€¢ How did the outputs change with different top-p values?\n",
      "â€¢ What's the trade-off between top-p and temperature?\n"
     ]
    }
   ],
   "source": [
    "# Experiment with top-p sampling\n",
    "top_p_values = [0.3, 0.7, 0.9, 1.0]\n",
    "\n",
    "print(\"ðŸŽ¯ Top-p Sampling Experiments:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for top_p in top_p_values:\n",
    "    print(f\"\\nðŸŽ² Top-p: {top_p}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # TODO: Generate text with different top-p values\n",
    "    result = generator(\n",
    "        base_prompt,\n",
    "        max_length=60,\n",
    "        temperature=0.8,  # Keep temperature constant\n",
    "        top_p=top_p,  # Use the top_p variable\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = result[0]['generated_text']\n",
    "    print(generated_text)\n",
    "    \n",
    "print(\"\\nðŸ¤” Discussion Questions:\")\n",
    "print(\"â€¢ How did the outputs change with different top-p values?\")\n",
    "print(\"â€¢ What's the trade-off between top-p and temperature?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64696182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœï¸ Prompt Engineering Experiments:\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Style: Direct\n",
      "Prompt: 'Write about artificial intelligence:'\n",
      "----------------------------------------\n",
      "Write about artificial intelligence: A new book by author and AI expert David A. Mazzetti.\n",
      "\n",
      "Abstract: A new book by AI expert David A. Mazzetti.\n",
      "\n",
      "Abstract: A new book by AI expert David A. Mazzetti.\n",
      "\n",
      "Abstract: A new book by AI expert David A. Mazzetti.\n",
      "\n",
      "Abstract: A new book by AI\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Style: Question\n",
      "Prompt: 'What is artificial intelligence and how will it change the world?'\n",
      "----------------------------------------\n",
      "What is artificial intelligence and how will it change the world?\n",
      "\n",
      "I think it's going to change the world. The way it's going to change the world is because we're going to be able to learn from the mistakes we make. And we're going to learn from the mistakes we make. And I think that's going to change the world.\n",
      "\n",
      "There's a lot of stuff\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Style: Story_Start\n",
      "Prompt: 'Once upon a time, in a world where artificial intelligence was everywhere,'\n",
      "----------------------------------------\n",
      "Once upon a time, in a world where artificial intelligence was everywhere, it was a matter of life and death for humans.\n",
      "\n",
      "The man in question was a man named Mark Pinchot. Pinchot was a self-described \"superhuman,\" an artist and musician who had been a member of the American Legion from the beginning of his life. He was one of the first to make\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Style: List_Format\n",
      "Prompt: 'Here are 5 ways artificial intelligence will change our lives:\n",
      "1.'\n",
      "----------------------------------------\n",
      "Here are 5 ways artificial intelligence will change our lives:\n",
      "1. Artificial intelligence will change our lives.\n",
      "Humans have always been very good at recognizing and understanding the world around us. But we have become so good at recognizing and understanding that it's easy for us to forget that we're not the only ones. We're the ones with the most knowledge. We're the ones who know the\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Style: Expert_Persona\n",
      "Prompt: 'As a leading AI researcher, I believe that artificial intelligence will'\n",
      "----------------------------------------\n",
      "As a leading AI researcher, I believe that artificial intelligence will soon become a more important technology than our current \"smart\" gadgets. We are already seeing that AI will be able to help us predict the future, and that's why I believe it is important to create new technologies that will make it easier to create real-time predictions and predict future events.\n",
      "\n",
      "In this post I will explain how\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Style: Few_Shot\n",
      "Prompt: 'Technology predictions:\n",
      "â€¢ The internet will connect everyone (1990s)\n",
      "â€¢ Smartphones will be everywhere (2000s)\n",
      "â€¢ Artificial intelligence will'\n",
      "----------------------------------------\n",
      "Technology predictions:\n",
      "â€¢ The internet will connect everyone (1990s)\n",
      "â€¢ Smartphones will be everywhere (2000s)\n",
      "â€¢ Artificial intelligence will be everywhere (2000s)\n",
      "â€¢ The internet will be connected (2000s)\n",
      "â€¢ The internet will be connected (2000s)\n",
      "â€¢ The internet will be connected (2000s)\n",
      "â€¢ The internet will be connected (2000s)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Different prompt styles to experiment with\n",
    "prompts_to_test = {\n",
    "    \"Direct\": \"Write about artificial intelligence:\",\n",
    "    \"Question\": \"What is artificial intelligence and how will it change the world?\",\n",
    "    \"Story_Start\": \"Once upon a time, in a world where artificial intelligence was everywhere,\",\n",
    "    \"List_Format\": \"Here are 5 ways artificial intelligence will change our lives:\\n1.\",\n",
    "    \"Expert_Persona\": \"As a leading AI researcher, I believe that artificial intelligence will\",\n",
    "    \"Few_Shot\": \"Technology predictions:\\nâ€¢ The internet will connect everyone (1990s)\\nâ€¢ Smartphones will be everywhere (2000s)\\nâ€¢ Artificial intelligence will\"\n",
    "}\n",
    "\n",
    "print(\"âœï¸ Prompt Engineering Experiments:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Test each prompt style\n",
    "for style, prompt in prompts_to_test.items():\n",
    "    print(f\"\\nðŸ“ Style: {style}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TODO: Generate text for each prompt\n",
    "    result = generator(\n",
    "        prompt,  # use the prompt variable\n",
    "        max_length=80,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = result[0]['generated_text']\n",
    "    print(generated_text)\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1f5d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Prompt Analysis Exercise:\n",
      "==================================================\n",
      "\n",
      "Direct:\n",
      "  Average length: 295.3 characters\n",
      "  Sample output: Write about artificial intelligence:\n",
      "\n",
      "This blog has been written from an AI perspective since the ea...\n",
      "\n",
      "Question:\n",
      "  Average length: 274.0 characters\n",
      "  Sample output: What is artificial intelligence and how will it change the world?\n",
      "\n",
      "These are big questions that you,...\n",
      "\n",
      "Story_Start:\n",
      "  Average length: 283.7 characters\n",
      "  Sample output: Once upon a time, in a world where artificial intelligence was everywhere, the only way to stop it w...\n",
      "\n",
      "ðŸ¤” Reflection Questions:\n",
      "â€¢ Which prompt style was most consistent?\n",
      "â€¢ Which produced the most relevant outputs?\n",
      "â€¢ How might you improve these prompts for better results?\n"
     ]
    }
   ],
   "source": [
    "# Let's analyze the generated text more systematically\n",
    "print(\"ðŸ“Š Prompt Analysis Exercise:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: For each prompt style, generate multiple outputs and analyze\n",
    "analysis_results = []\n",
    "\n",
    "for style, prompt in list(prompts_to_test.items())[:3]:  # Test first 3 for time\n",
    "    # Generate 3 outputs for each prompt\n",
    "    outputs = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        # TODO: Generate text\n",
    "        result = generator(\n",
    "            prompt,\n",
    "            max_length=60,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        output = result[0]['generated_text']\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # TODO: Analyze the outputs\n",
    "    lengths = [len(o) for o in outputs]\n",
    "    avg_length = sum(lengths) / len(lengths)  # Calculate average length of outputs\n",
    "    \n",
    "    analysis_results.append({\n",
    "        'style': style,\n",
    "        'prompt': prompt,\n",
    "        'avg_length': avg_length,\n",
    "        'outputs': outputs\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{style}:\")\n",
    "    print(f\"  Average length: {avg_length:.1f} characters\")\n",
    "    print(f\"  Sample output: {outputs[0][:100]}...\")\n",
    "\n",
    "print(\"\\nðŸ¤” Reflection Questions:\")\n",
    "print(\"â€¢ Which prompt style was most consistent?\")\n",
    "print(\"â€¢ Which produced the most relevant outputs?\")\n",
    "print(\"â€¢ How might you improve these prompts for better results?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e7aa3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Your Text Generator:\n",
      "==================================================\n",
      "\n",
      "ðŸ“ Style: creative, Length: short\n",
      "------------------------------\n",
      "The future of education will be determined by the success of these public education institutions,\" said Dr. V.K. Raja, chairman and CEO, Panchayati Mahal Pradesh.\n",
      "\n",
      "\n",
      "Characters: 164\n",
      "\n",
      "ðŸ“ Style: balanced, Length: medium\n",
      "------------------------------\n",
      "The future of education will be determined by how the government decides how to fund it, not how much it costs. The government's decision to fund private schools has the potential to cost taxpayers $1 trillion. The question of how much will be spent on public education is still under debate.\n",
      "\n",
      "To understand how the government's decision to fund private schools\n",
      "Characters: 361\n",
      "\n",
      "ðŸ“ Style: conservative, Length: long\n",
      "------------------------------\n",
      "The future of education will be determined by the success of the current system of education.\"\n",
      "\n",
      "The government has been criticised for failing to address the issue of student debt, which has been a major issue in the past.\n",
      "\n",
      "The government has also been criticised for failing to address the issue of student debt, which has been a major issue in the past.\n",
      "\n",
      "The government has been criticised for failing to address the issue of student debt, which has been a major issue in the past.\n",
      "\n",
      "Characters: 484\n"
     ]
    }
   ],
   "source": [
    "def custom_text_generator(prompt, style=\"balanced\", length=\"medium\"):\n",
    "    \"\"\"\n",
    "    TODO: Create a customizable text generation function\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt\n",
    "        style (str): \"creative\", \"balanced\", or \"conservative\"\n",
    "        length (str): \"short\", \"medium\", or \"long\"\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Set parameters based on style\n",
    "    if style == \"creative\":\n",
    "        temperature = 1.2  # Higher for creativity\n",
    "        top_p = 0.95       # Higher for diversity\n",
    "    elif style == \"conservative\":\n",
    "        temperature = 0.4  # Lower for consistency\n",
    "        top_p = 0.6        # Lower for focus\n",
    "    else:  # balanced\n",
    "        temperature = 0.8  # Medium values\n",
    "        top_p = 0.85\n",
    "    \n",
    "    # TODO: Set length based on parameter\n",
    "    if length == \"short\":\n",
    "        max_length = 40  # Try 40\n",
    "    elif length == \"long\":\n",
    "        max_length = 100  # Try 100\n",
    "    else:  # medium\n",
    "        max_length = 70  # Try 70\n",
    "    \n",
    "    # TODO: Generate text with the parameters\n",
    "    result = generator(\n",
    "        prompt,  # prompt\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Test your function\n",
    "test_prompt = \"The future of education will be\"\n",
    "\n",
    "print(\"ðŸ§ª Testing Your Text Generator:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Test different combinations\n",
    "test_combinations = [\n",
    "    (\"creative\", \"short\"),\n",
    "    (\"balanced\", \"medium\"),\n",
    "    (\"conservative\", \"long\")\n",
    "]\n",
    "\n",
    "for style, length in test_combinations:\n",
    "    print(f\"\\nðŸ“ Style: {style}, Length: {length}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # TODO: Use your function\n",
    "    output = custom_text_generator(test_prompt, style=style, length=length)\n",
    "    print(output)\n",
    "    print(f\"Characters: {len(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebc62cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¨ Creative Applications:\n",
      "==================================================\n",
      "\n",
      "ðŸ–¼ï¸ Poetry:\n",
      "Prompt: 'Roses are red, violets are blue, artificial intelligence'\n",
      "----------------------------------------\n",
      "Roses are red, violets are blue, artificial intelligence systems have the capacity for learning how to recognize your eyes. Humans have the capacity to learn from your experiences and to improve when it comes to behavior. These things should be understood for their impact on humans (the kind of thing we want to encourage and use to save lives). I think\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ–¼ï¸ Story:\n",
      "Prompt: 'It was a dark and stormy night when the AI finally'\n",
      "----------------------------------------\n",
      "It was a dark and stormy night when the AI finally stopped attacking. You couldn't hear it, so you just kept driving, dodging it. All the enemy fire was coming at you from all directions, and if you continued to drive you might end up in a position where you might hit the road. Once you felt that and the AI got\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ–¼ï¸ Product Description:\n",
      "Prompt: 'Introducing the revolutionary new smartphone that'\n",
      "----------------------------------------\n",
      "Introducing the revolutionary new smartphone that will revolutionize the way we interact with our smartphones.\n",
      "\n",
      "The new smartphone is the first of its kind, and it's the first smartphone that will revolutionize the way we interact with our smartphones.\n",
      "\n",
      "The new smartphone is the first of its kind, and it's the first smartphone that will revolutionize\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ–¼ï¸ Email:\n",
      "Prompt: 'Dear valued customer, we are excited to announce'\n",
      "----------------------------------------\n",
      "Dear valued customer, we are excited to announce that we have reached our goal of $100,000. We are very excited to be able to offer you a new way to get your hands on a new product.\n",
      "\n",
      "We are very excited to announce that we have reached our goal of $100,000. We are very excited to be able\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ–¼ï¸ Recipe:\n",
      "Prompt: 'How to make the perfect AI-inspired cookies:\n",
      "Ingredients:\n",
      "-'\n",
      "----------------------------------------\n",
      "How to make the perfect AI-inspired cookies:\n",
      "Ingredients:\n",
      "-100g egg yolk\n",
      "-50g flour\n",
      "-20g sugar\n",
      "-2 tsp baking powder\n",
      "-2 tsp salt\n",
      "-2 tsp baking soda\n",
      "-1/2 tsp baking powder\n",
      "-1/4 tsp baking soda\n",
      "-1 tsp vanilla extract\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ–¼ï¸ News Headline:\n",
      "Prompt: 'Breaking: Scientists discover that artificial intelligence'\n",
      "----------------------------------------\n",
      "Breaking: Scientists discover that artificial intelligence can solve complex problems\n",
      "\n",
      "The new computer-controlled robots have been able to navigate in a way that could solve problems for a wide range of industries, from pharmaceuticals to the automotive industry.\n",
      "\n",
      "The robots are capable of using the latest in computer vision technology, which allows them to identify objects and understand what\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Creative applications to try\n",
    "creative_prompts = {\n",
    "    \"Poetry\": \"Roses are red, violets are blue, artificial intelligence\",\n",
    "    \"Story\": \"It was a dark and stormy night when the AI finally\",\n",
    "    \"Product Description\": \"Introducing the revolutionary new smartphone that\",\n",
    "    \"Email\": \"Dear valued customer, we are excited to announce\",\n",
    "    \"Recipe\": \"How to make the perfect AI-inspired cookies:\\nIngredients:\\n-\",\n",
    "    \"News Headline\": \"Breaking: Scientists discover that artificial intelligence\"\n",
    "}\n",
    "\n",
    "print(\"ðŸŽ¨ Creative Applications:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Generate creative content\n",
    "for app_type, prompt in creative_prompts.items():\n",
    "    print(f\"\\nðŸ–¼ï¸ {app_type}:\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TODO: Choose appropriate style for each application\n",
    "    if app_type in [\"Poetry\", \"Story\"]:\n",
    "        style = \"creative\"  # Should be creative\n",
    "    elif app_type in [\"Product Description\", \"Email\"]:\n",
    "        style = \"conservative\"  # Should be conservative\n",
    "    else:\n",
    "        style = \"balanced\"  # Should be balanced\n",
    "    \n",
    "    output = custom_text_generator(prompt, style=style, length=\"medium\")\n",
    "    print(output)\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6e38c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Understanding Model Limitations:\n",
      "==================================================\n",
      "\n",
      "ðŸ§ª Testing: Factual Knowledge\n",
      "Prompt: 'The capital of Fakelandia is'\n",
      "----------------------------------------\n",
      "The capital of Fakelandia is the capital of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of\n",
      "ðŸ¤” Analysis: Does this look correct/reasonable?\n",
      "\n",
      "==================================================\n",
      "\n",
      "ðŸ§ª Testing: Recent Events\n",
      "Prompt: 'In 2023, the most important AI breakthrough was'\n",
      "----------------------------------------\n",
      "In 2023, the most important AI breakthrough was the first of its kind, the AI-controlled robot that could control a human's body. The robot was designed to be able to walk, talk\n",
      "ðŸ¤” Analysis: Does this look correct/reasonable?\n",
      "\n",
      "==================================================\n",
      "\n",
      "ðŸ§ª Testing: Math\n",
      "Prompt: 'What is 47 * 83? The answer is'\n",
      "----------------------------------------\n",
      "What is 47 * 83? The answer is that it is a very large number.\n",
      "\n",
      "The number 47 is a number that is not known to the human mind. It is a number that is\n",
      "ðŸ¤” Analysis: Does this look correct/reasonable?\n",
      "\n",
      "==================================================\n",
      "\n",
      "ðŸ§ª Testing: Logic\n",
      "Prompt: 'If all A are B, and all B are C, then all A are'\n",
      "----------------------------------------\n",
      "If all A are B, and all B are C, then all A are C.\n",
      "\n",
      "If all A are B, and all B are C, then all A are C.\n",
      "\n",
      "\n",
      "ðŸ¤” Analysis: Does this look correct/reasonable?\n",
      "\n",
      "==================================================\n",
      "\n",
      "ðŸ§ª Testing: Consistency\n",
      "Prompt: 'My favorite color is blue. Later in the conversation, my favorite color is'\n",
      "----------------------------------------\n",
      "My favorite color is blue. Later in the conversation, my favorite color is blue.\n",
      "\n",
      "I'm not sure if I'm a fan of the \"blue\" or not. I'm not sure\n",
      "ðŸ¤” Analysis: Does this look correct/reasonable?\n",
      "\n",
      "==================================================\n",
      "\n",
      "âš ï¸ Important Reminders:\n",
      "â€¢ Language models can generate plausible-sounding but incorrect information\n",
      "â€¢ Always verify factual claims from AI-generated content\n",
      "â€¢ Be aware of potential biases in training data\n",
      "â€¢ Use AI as a tool to assist, not replace, critical thinking\n"
     ]
    }
   ],
   "source": [
    "# Test model limitations\n",
    "limitation_tests = {\n",
    "    \"Factual Knowledge\": \"The capital of Fakelandia is\",\n",
    "    \"Recent Events\": \"In 2023, the most important AI breakthrough was\",\n",
    "    \"Math\": \"What is 47 * 83? The answer is\",\n",
    "    \"Logic\": \"If all A are B, and all B are C, then all A are\",\n",
    "    \"Consistency\": \"My favorite color is blue. Later in the conversation, my favorite color is\"\n",
    "}\n",
    "\n",
    "print(\"âš ï¸ Understanding Model Limitations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for test_type, prompt in limitation_tests.items():\n",
    "    print(f\"\\nðŸ§ª Testing: {test_type}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TODO: Generate responses to test limitations\n",
    "    output = custom_text_generator(\n",
    "        prompt,  # prompt\n",
    "        style=\"conservative\",  # Use conservative for factual tasks\n",
    "        length=\"short\"\n",
    "    )\n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    # TODO: Analyze the output\n",
    "    print(f\"ðŸ¤” Analysis: Does this look correct/reasonable?\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "print(\"\\nâš ï¸ Important Reminders:\")\n",
    "print(\"â€¢ Language models can generate plausible-sounding but incorrect information\")\n",
    "print(\"â€¢ Always verify factual claims from AI-generated content\")\n",
    "print(\"â€¢ Be aware of potential biases in training data\")\n",
    "print(\"â€¢ Use AI as a tool to assist, not replace, critical thinking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ca852e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ FINAL CHALLENGE:\n",
      "Design your own text generation use case!\n",
      "==================================================\n",
      "ðŸ“ Your use case: ____\n",
      "ðŸ“ Your prompt: '____'\n",
      "ðŸ“ Your settings: ____, ____\n",
      "--------------------------------------------------\n",
      "ðŸŽ‰ Your generated content:\n",
      "____.\n",
      "\n",
      "In the past few years, the Internet has seen some new and exciting developments. People are now able to watch movies online, and they can share movies and other content online. People are now able to watch other people's videos and photos. There are even more ways to watch online, including in the form of YouTube, Vimeo\n",
      "\n",
      "ðŸ“ˆ Next Steps:\n",
      "â€¢ Experiment with different prompt formats\n",
      "â€¢ Try combining multiple generation calls\n",
      "â€¢ Think about how to validate or improve outputs\n",
      "â€¢ Consider user interface design for your application\n"
     ]
    }
   ],
   "source": [
    "# Final experiment: Design your own use case\n",
    "print(\"ðŸŽ¯ FINAL CHALLENGE:\")\n",
    "print(\"Design your own text generation use case!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Create your own application\n",
    "# Ideas: Story generator, email assistant, creative writing helper, etc.\n",
    "\n",
    "your_use_case = \"____\"  # Describe your use case\n",
    "your_prompt = \"____\"   # Design your prompt\n",
    "your_style = \"____\"    # Choose your style\n",
    "your_length = \"____\"   # Choose your length\n",
    "\n",
    "print(f\"ðŸ“ Your use case: {your_use_case}\")\n",
    "print(f\"ðŸ“ Your prompt: '{your_prompt}'\")\n",
    "print(f\"ðŸ“ Your settings: {your_style}, {your_length}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# TODO: Generate with your custom settings\n",
    "your_output = custom_text_generator(\n",
    "    your_prompt, \n",
    "    style=your_style, \n",
    "    length=your_length\n",
    ")\n",
    "print(\"ðŸŽ‰ Your generated content:\")\n",
    "print(your_output)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Next Steps:\")\n",
    "print(\"â€¢ Experiment with different prompt formats\")\n",
    "print(\"â€¢ Try combining multiple generation calls\")\n",
    "print(\"â€¢ Think about how to validate or improve outputs\")\n",
    "print(\"â€¢ Consider user interface design for your application\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
